{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I_ScQt3ellpn"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Ye8dD0llpq"
      },
      "source": [
        "训练一个玛丽奥玩家的强化学习代理\n",
        "作者: Yuansong Feng, Suraj Subramanian, Howard Wang, Steven Guo。\n",
        "\n",
        "本教程将引导您了解深度强化学习的基础知识。最终，您将实现一个可以自主玩游戏的AI驱动的玛丽奥（使用Double Deep Q-Networks_）。\n",
        "\n",
        "尽管此教程无需强化学习的先前知识，但您可以通过这些强化学习的概念进行熟悉，并随时使用这个速查表 作为您的参考。完整的代码可在这里_获取。\n",
        "\n",
        ".. 图片:: /_static/img/mario.gif\n",
        ":alt: 玛丽奥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FeGUHjx1llpt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6XKZ6tOllpu"
      },
      "source": [
        "强化学习定义\n",
        "环境（Environment）：代理与之交互并从中学习的世界。\n",
        "\n",
        "动作（Action） \n",
        "�\n",
        "a：代理如何响应环境。所有可能的动作集合被称为动作空间。\n",
        "\n",
        "状态（State） \n",
        "�\n",
        "s：环境的当前特征。环境可能处于的所有可能状态的集合被称为状态空间。\n",
        "\n",
        "奖励（Reward） \n",
        "�\n",
        "r：奖励是环境给代理的关键反馈。这是驱使代理学习并改变其未来动作的因素。在多个时间步骤上的奖励汇总被称为回报（Return）。\n",
        "\n",
        "最优动作-价值函数（Optimal Action-Value function） \n",
        "�\n",
        "∗\n",
        "(\n",
        "�\n",
        ",\n",
        "�\n",
        ")\n",
        "Q \n",
        "∗\n",
        " (s,a)：给出了如果您从状态 \n",
        "�\n",
        "s 开始，采取任意动作 \n",
        "�\n",
        "a ，然后对于每个未来时间步骤采取最大化回报的动作的预期回报。\n",
        "�\n",
        "Q 可以被认为是状态中动作的“质量”。我们尝试逼近这个函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cepVuyHBllpv"
      },
      "source": [
        "环境\n",
        "初始化环境\n",
        "在玛丽奥游戏中，环境由管道、蘑菇和其他组件构成。\n",
        "\n",
        "当玛丽奥执行一个动作时，环境会响应并返回改变后的（下一个）状态、奖励以及其他信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eTqERNPllpv",
        "outputId": "02e5d762-1f2d-48d9-ff78-875b43679be0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\envs\\mario\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "d:\\Anaconda3\\envs\\mario\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\envs\\mario\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True,render_mode='human')\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA-lJnfgllpw"
      },
      "source": [
        "环境预处理\n",
        "环境数据以 next_state 的形式返回给代理。如上所示，每个状态由一个大小为 [3, 240, 256] 的数组表示。通常，这比我们的代理需要的信息要多；例如，玛丽奥的行动并不依赖于管道或天空的颜色！\n",
        "\n",
        "我们使用 包装器（Wrappers） 在将数据发送给代理之前对环境数据进行预处理。\n",
        "\n",
        "GrayScaleObservation 是一个常见的包装器，用于将RGB图像转换为灰度图像；这样做可以减小状态表示的大小，而不会丢失有用的信息。现在每个状态的大小为： [1, 240, 256]\n",
        "\n",
        "ResizeObservation 将每个观察结果缩小到一个正方形图像中。新的大小为： [1, 84, 84]\n",
        "\n",
        "SkipFrame 是一个自定义的包装器，继承自 gym.Wrapper 并实现了 step() 函数。由于连续的帧差异不大，我们可以跳过n个中间帧而不会丢失太多信息。第n帧会累计每个跳过帧上累积的奖励。\n",
        "\n",
        "FrameStack 是一个包装器，允许我们将环境的连续帧压缩成单个观察点，以供我们的学习模型使用。这样，我们可以根据玛丽奥在前几帧中的移动方向来确定他是降落还是跳跃。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ua9t54lhllpx"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"每隔 `skip` 帧返回一次\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"重复动作并累计奖励\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # 累计奖励并重复相同的动作\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # 重排 [H, W, C] 数组为 [C, H, W] 张量\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# 将包装器应用于环境\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-3i_xuxllpx"
      },
      "source": [
        "经过上述包装器对环境的应用后，最终的包装状态由4个连续的灰度帧堆叠在一起组成，如上图左侧所示。每当玛丽奥执行一个动作时，环境都会以这种结构的状态进行响应。这种结构由一个大小为[4, 84, 84]的3D数组表示。\n",
        "\n",
        ".. 图片:: /_static/img/mario_env.png\n",
        ":alt: 图片\n",
        "\n",
        "这段描述解释了经过处理后的环境状态是如何由四个灰度帧堆叠而成，并给出了相应的图像以进一步说明这一概念。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69RAEmVnllpy"
      },
      "source": [
        "代理\n",
        "我们创建一个名为Mario的类来代表游戏中的代理。玛丽奥应该能够：\n",
        "\n",
        "根据当前状态（环境的） 执行基于最优动作策略的动作。\n",
        "\n",
        "记住 经验。经验 = （当前状态，当前动作，奖励，下一个状态）。玛丽奥会缓存并稍后回忆他的经验以更新他的动作策略。\n",
        "\n",
        "随着时间的推移学习 更好的动作策略。\n",
        "\n",
        "这段描述概述了代理的三个主要功能：根据当前状态执行动作、记住和回忆经验，以及随时间学习并改进其策略。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rv39gzvzllpy"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"给定一个状态，选择一个ε-贪婪（epsilon-greedy）动作\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"将经验添加到内存中\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"从内存中抽样经验\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"使用一批经验更新在线动作值（Q值）函数\"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72-nHycgllpy"
      },
      "source": [
        "\n",
        "在接下来的部分中，我们将填充玛丽奥的参数并定义他的函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FNFbG9Allpz"
      },
      "source": [
        "\n",
        "执行\n",
        "对于任何给定的状态，代理可以选择执行最优的动作（利用）或随机的动作（探索）。\n",
        "\n",
        "玛丽奥有一个概率 self.exploration_rate 来进行随机探索；当他选择利用时，他依赖于 MarioNet（在学习部分中实现）来提供最优的动作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4AA7k4M8llpz"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        # 初始化玛丽奥的状态维度、动作维度和保存目录\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # 检测设备是否支持CUDA，如果支持，则使用CUDA，否则使用CPU\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # 初始化Mario的DNN以预测最优动作 - 我们在后面的“学习”部分实现这一部分\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        # 设置探索率相关的参数\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        # 每隔多少经验保存一次Mario Net\n",
        "        self.save_every = 5e5  \n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        给定一个状态，选择一个ε-贪婪动作并更新步骤的值。\n",
        "        \n",
        "        输入:\n",
        "        state(``LazyFrame``): 当前状态的单个观察值，维度为 (state_dim)\n",
        "        \n",
        "        输出:\n",
        "        ``action_idx`` (``int``): 表示玛丽奥将执行的动作的整数索引\n",
        "        \"\"\"\n",
        "        # 探索\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "        # 利用\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # 减少探索率\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # 增加步骤\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxXpuF9mllpz"
      },
      "source": [
        "缓存和回忆\n",
        "这两个函数充当玛丽奥的“记忆”过程。\n",
        "\n",
        "cache()：每次玛丽奥执行一个动作时，他都会将experience（经验）存储到他的记忆中。他的经验包括当前的状态、执行的动作、从动作中得到的奖励、下一个状态以及游戏是否完成。\n",
        "\n",
        "recall()：玛丽奥从他的记忆中随机抽样一批经验，并使用这些经验来学习游戏。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vrRWomPzllpz"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):  # 继承父类以保持连续性\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        # 初始化记忆回放缓冲区\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 32  # 批处理大小\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        将经验存储到self.memory（回放缓冲区）中\n",
        "\n",
        "        输入:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "        # 辅助函数，如果输入是元组，则取第一个元素\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        \n",
        "        # 将LazyFrame转换为数组\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        # 转换为PyTorch张量\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        # 将经验添加到记忆中\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        从记忆中检索一批经验\n",
        "        \"\"\"\n",
        "        # 从记忆中随机抽样一批经验，并将其移到适当的设备上\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        \n",
        "        # 获取批量中的各个经验项\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        \n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8omgYALWllpz"
      },
      "source": [
        "学习\n",
        "玛丽奥在其内部使用了DDQN算法。DDQN使用两个卷积神经网络 - \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "Q \n",
        "online\n",
        "​\n",
        "  和 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "Q \n",
        "target\n",
        "​\n",
        " ，它们独立地逼近最优动作值函数。\n",
        "\n",
        "在我们的实现中，我们跨 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "Q \n",
        "online\n",
        "​\n",
        "  和 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "Q \n",
        "target\n",
        "​\n",
        "  共享特征生成器 features，但为每个网络维护独立的全连接分类器。为了防止通过反向传播更新，\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "θ \n",
        "target\n",
        "​\n",
        " （\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "Q \n",
        "target\n",
        "​\n",
        "  的参数）被冻结。相反，它会定期与 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "θ \n",
        "online\n",
        "​\n",
        "  同步（稍后会详细说明）。\n",
        "\n",
        "神经网络\n",
        "接下来将讨论神经网络的具体实现和细节。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CSpWc5HSllpz"
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"迷你CNN结构\n",
        "  输入 -> (conv2d + relu) x 3 -> 展平 -> (dense + relu) x 2 -> 输出\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"期望输入的高度为: 84, 实际为: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"期望输入的宽度为: 84, 实际为: {w}\")\n",
        "\n",
        "        # 构建在线模型和目标模型\n",
        "        self.online = self.__build_cnn(c, output_dim)\n",
        "        self.target = self.__build_cnn(c, output_dim)\n",
        "        \n",
        "        # 将在线模型的权重加载到目标模型中\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # 冻结目标模型的参数，不进行反向传播更新\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        \"\"\"前向传播函数，根据模型参数决定是在线模型还是目标模型\"\"\"\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)\n",
        "\n",
        "    def __build_cnn(self, c, output_dim):\n",
        "        \"\"\"构建CNN结构\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBscmqYGllp0"
      },
      "source": [
        "\n",
        "TD 估计与 TD 目标\n",
        "在学习过程中涉及到两个值：\n",
        "\n",
        "TD 估计 - 对于给定状态 \n",
        "�\n",
        "s，预测的最优 \n",
        "�\n",
        "∗\n",
        "Q \n",
        "∗\n",
        "  值为：\n",
        "\n",
        "�\n",
        "�\n",
        "�\n",
        "=\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "∗\n",
        "(\n",
        "�\n",
        ",\n",
        "�\n",
        ")\n",
        "TD \n",
        "e\n",
        "​\n",
        " =Q \n",
        "online\n",
        "∗\n",
        "​\n",
        " (s,a)\n",
        "TD 目标 - 当前奖励和下一个状态 \n",
        "�\n",
        "′\n",
        "s \n",
        "′\n",
        "  中估计的 \n",
        "�\n",
        "∗\n",
        "Q \n",
        "∗\n",
        "  值的聚合，计算方法如下：\n",
        "\n",
        "选择在下一个状态 \n",
        "�\n",
        "′\n",
        "s \n",
        "′\n",
        "  中使得 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "(\n",
        "�\n",
        "′\n",
        ",\n",
        "�\n",
        ")\n",
        "Q \n",
        "online\n",
        "​\n",
        " (s \n",
        "′\n",
        " ,a) 最大化的动作 \n",
        "�\n",
        "′\n",
        "a \n",
        "′\n",
        " ：\n",
        "�\n",
        "′\n",
        "=\n",
        "argmax\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "(\n",
        "�\n",
        "′\n",
        ",\n",
        "�\n",
        ")\n",
        "a \n",
        "′\n",
        " =argmax \n",
        "a\n",
        "​\n",
        " Q \n",
        "online\n",
        "​\n",
        " (s \n",
        "′\n",
        " ,a)\n",
        "计算 TD 目标：\n",
        "�\n",
        "�\n",
        "�\n",
        "=\n",
        "�\n",
        "+\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "∗\n",
        "(\n",
        "�\n",
        "′\n",
        ",\n",
        "�\n",
        "′\n",
        ")\n",
        "TD \n",
        "t\n",
        "​\n",
        " =r+γQ \n",
        "target\n",
        "∗\n",
        "​\n",
        " (s \n",
        "′\n",
        " ,a \n",
        "′\n",
        " )\n",
        "由于我们不知道下一个动作 \n",
        "�\n",
        "′\n",
        "a \n",
        "′\n",
        "  会是什么，因此在下一个状态 \n",
        "�\n",
        "′\n",
        "s \n",
        "′\n",
        "  中我们使用最大化 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "Q \n",
        "online\n",
        "​\n",
        "  的动作 \n",
        "�\n",
        "′\n",
        "a \n",
        "′\n",
        " 。\n",
        "\n",
        "值得注意的是，我们在 td_target() 方法上使用了 @torch.no_grad() 装饰器来禁用此处的梯度计算（因为我们不需要在 \n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "�\n",
        "θ \n",
        "target\n",
        "​\n",
        "  上进行反向传播）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_MLFixhJllp0"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.gamma = 0.9  # 定义折扣因子为 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        \"\"\"\n",
        "        计算TD估计值\n",
        "        \"\"\"\n",
        "        # 获取当前动作对应的在线模型下的Q值\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        计算TD目标值\n",
        "        \"\"\"\n",
        "        # 获取下一个状态在在线模型下的Q值\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        \n",
        "        # 选择在下一个状态中最优的动作\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        \n",
        "        # 获取目标模型下的Q值\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        \n",
        "        # 计算TD目标值\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qjY3tGvllp0"
      },
      "source": [
        "更新模型\n",
        "当玛丽奥从其回放缓冲区中采样输入时，我们计算 \n",
        "�\n",
        "�\n",
        "�\n",
        "TD \n",
        "t\n",
        "​\n",
        "  和 \n",
        "�\n",
        "�\n",
        "�\n",
        "TD \n",
        "e\n",
        "​\n",
        " ，然后反向传播这个损失到 \n",
        "�\n",
        "online\n",
        "Q \n",
        "online\n",
        "​\n",
        " ，以更新其参数 \n",
        "�\n",
        "online\n",
        "θ \n",
        "online\n",
        "​\n",
        " （\n",
        "�\n",
        "α 是传递给 optimizer 的学习率 lr）：\n",
        "\n",
        "�\n",
        "online\n",
        "←\n",
        "�\n",
        "online\n",
        "+\n",
        "�\n",
        "∇\n",
        "(\n",
        "�\n",
        "�\n",
        "�\n",
        "−\n",
        "�\n",
        "�\n",
        "�\n",
        ")\n",
        "θ \n",
        "online\n",
        "​\n",
        " ←θ \n",
        "online\n",
        "​\n",
        " +α∇(TD \n",
        "e\n",
        "​\n",
        " −TD \n",
        "t\n",
        "​\n",
        " )\n",
        "�\n",
        "target\n",
        "θ \n",
        "target\n",
        "​\n",
        "  不通过反向传播进行更新。相反，我们定期将 \n",
        "�\n",
        "online\n",
        "θ \n",
        "online\n",
        "​\n",
        "  复制到 \n",
        "�\n",
        "target\n",
        "θ \n",
        "target\n",
        "​\n",
        " ：\n",
        "\n",
        "�\n",
        "target\n",
        "←\n",
        "�\n",
        "online\n",
        "θ \n",
        "target\n",
        "​\n",
        " ←θ \n",
        "online\n",
        "​\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yFAc9T7Mllp0"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yia2lMQillp1"
      },
      "source": [
        "#### Save checkpoint\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WqKTyAYDllp1"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        \"\"\"\n",
        "        保存 MarioNet 模型和探索率\n",
        "        \"\"\"\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_weights(agent, path):\n",
        "    \"\"\"加载模型权重到代理网络\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        state_dict = torch.load(path)\n",
        "    else:\n",
        "        state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
        "    \n",
        "    agent.net.load_state_dict(state_dict['model'])\n",
        "    agent.exploration_rate = state_dict['exploration_rate']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPPk35tIllp1"
      },
      "source": [
        "综合所有内容\n",
        "在这里，我们将所有的内容整合在一起，包括环境的初始化、预处理、代理的定义、网络结构、学习算法以及模型的保存和加载等步骤。通过上述步骤，我们可以构建一个完整的玛丽奥强化学习代理，使其能够在游戏中学习和执行复杂的任务。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1Saey3wJllp1"
      },
      "outputs": [],
      "source": [
        "# 定义一个新的 Mario 类，该类从基础 Mario 类继承\n",
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        \n",
        "        # 设置开始训练前需要的最小经验数量\n",
        "        self.burnin = 1e4  \n",
        "        \n",
        "        # 每3个经验更新一次 Q_online\n",
        "        self.learn_every = 3  \n",
        "        \n",
        "        # 每1e4个经验，进行 Q_target 和 Q_online 的同步\n",
        "        self.sync_every = 1e4  \n",
        "\n",
        "    # 定义学习方法\n",
        "    def learn(self):\n",
        "        # 如果当前步数可以被 sync_every 整除，同步 Q_target 和 Q_online\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        # 如果当前步数可以被 save_every 整除，保存模型\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        # 如果当前步数小于 burnin，暂时不进行学习\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        # 如果当前步数不能被 learn_every 整除，暂时不进行学习\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # 从记忆中随机抽样获取经验数据\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # 计算 TD 估计值\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # 计算 TD 目标值\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # 通过 Q_online 反向传播并更新网络权重\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO1zImR6llp1"
      },
      "source": [
        "### Logging\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5Op6Om5Nllp1"
      },
      "outputs": [],
      "source": [
        "# 导入所需库\n",
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 定义 MetricLogger 类用于日志记录和性能度量\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        # 设置保存日志文件路径\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        # 在日志文件中写入列标题\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        \n",
        "        # 设置保存图像的路径\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # 初始化历史性能指标\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # 初始化移动平均指标\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # 初始化当前的回合指标\n",
        "        self.init_episode()\n",
        "\n",
        "        # 记录时间\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    # 记录每一步的信息\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        " \n",
        "    # 标记回合结束\n",
        "    def log_episode(self):\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "        self.init_episode()\n",
        "\n",
        "    # 初始化当前回合的指标\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    # 记录每一回合的性能指标\n",
        "    def record(self, episode, epsilon, step):\n",
        "        # 计算最近100回合的平均性能\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        # 计算时间差\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        # 打印当前回合的性能指标\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        # 在日志文件中追加当前回合的性能指标\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # 绘制并保存性能图像\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "des8WTL-llp2"
      },
      "source": [
        "让我们开始吧！\n",
        "在这个示例中，我们运行了40个回合的训练循环，但为了Mario真正学会他的世界，我们建议至少运行40,000个回合！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gewPfdolllp2",
        "outputId": "1a7ec190-6e77-4071-fee5-40bb917a6893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: False\n",
            "\n",
            "找到预训练模型：./saved_models/mario_net_22.chkpt\n",
            "已加载预训练模型\n"
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "# 是否加载预训练模型的标志\n",
        "load_pretrained_model = True # 设置为True或False，取决于你是否想要加载预训练模型\n",
        "# # 查找.pth文件\n",
        "# pretrained_model_path = None\n",
        "# for file_path in save_dir.glob(\"*.pth\"):\n",
        "#     pretrained_model_path = file_path\n",
        "#     break  # 找到第一个.pth文件后立即停止\n",
        "pretrained_model_path = \"./saved_models/mario_net_29.chkpt\"\n",
        "\n",
        "if load_pretrained_model:\n",
        "    # 检查是否找到了.pth文件\n",
        "    if pretrained_model_path:\n",
        "        print(f\"找到预训练模型：{pretrained_model_path}\")\n",
        "        load_model_weights(mario, pretrained_model_path)\n",
        "        print(\"已加载预训练模型\")\n",
        "    else:\n",
        "        print(\"未找到任何.pth文件\")\n",
        "    \n",
        "\n",
        "# 进行40个回合的训练\n",
        "episodes = 40\n",
        "for e in range(episodes):\n",
        "\n",
        "    # 重置环境状态\n",
        "    state = env.reset()\n",
        "\n",
        "    # 开始游戏循环\n",
        "    while True:\n",
        "\n",
        "        # 使代理基于当前状态做出行动\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # 代理执行动作\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # 记录经验\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # 学习和更新代理\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # 记录日志\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # 更新状态并渲染环境\n",
        "        state = next_state\n",
        "        #env.render()\n",
        "\n",
        "        # 检查游戏是否结束\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    # 记录每个回合的总结\n",
        "    logger.log_episode()\n",
        "\n",
        "    # 每20个回合或最后一个回合时记录和打印日志\n",
        "    if (e % 20 == 0) or (e == episodes - 1):\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
